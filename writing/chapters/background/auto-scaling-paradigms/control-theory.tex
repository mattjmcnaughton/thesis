Our next auto-scaling technique is predicated on control
theory. Control theory is normally used for reactive auto-scaling, although it
can also be used in a predictive context. The simplest implementation of a
control system with respect to auto-scaling utilizes feedback
controllers
\cite{auto-scaling-techniques-for-elastic-applications-in-cloud-environments}.
Abstractly, a feedback model functions by continuously examining a set of output parameters,
and then tweaking a set of desired input parameters in an attempt to ensure the
output parameters maintain some desired state. More concretely with respect to
auto-scaling, the output parameters would be the current state of the
application instances, such as the percent CPU utilization or the amount of
memory the instances were using. The input parameters would be the
number of instances of the application currently running. A feedback model
can implement auto-scaling as the number of application instances will vary in
accordance to the external load on the application, which will ensure that the
application instances maintain certain operation metrics. For example, we could
specify that the feedback controller should auto-scale applications such that
all application instances utilize $70\%$ of the CPU.

Done correctly, feedback control theory offers substantial advantages over
threshold-based rules. Specifically, it is as simple to write auto-scaling
specifications with control theory as it is to write specification with
threshold-based policies, as in both the author simply defines well-understood
resource metrics. Yet, it is easier to determine the effects of feedback
control systems. When a new instance is created as the result of the violation
of a threshold-based rule, we do not exactly know what the result will be with
respect to the metrics we care about. However, with a feedback control system,
we are certain about the results of the auto-scaling, as we auto-scale
specifically to ensure the maintenance of certain metrics.

Kubernetes currently implements auto-scaling through a feedback control system.
While we will spend substantially more time discussing the Kubernetes auto-scaling
implementation later, the basics are as follows. The user specifies a target resource
metric, for example CPU utilization. At a specified time interval,
Kubernetes then examines the current values
of the resource metric, and updates the number of application instances to
ensure the current actual value equals the target
value \cite{k8s-horizontal-pod-autoscaler-proposal}. In the context of control
theory, the output is the CPU utilization for each machine and the input is the
number of application instances, which varies to ensure the output is at the
proper level. Using this method, it is possible to auto-scale such that the
application is always running our chosen percent CPU utilization.

\subsubsection{Predictive Feedback Control}

\input{chapters/background/auto-scaling-paradigms/control-theory/predictive-feedback-control}
