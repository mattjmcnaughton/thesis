We define efficient resource utilization as a measure of whether an application
has enough, but not too many resources given to it by the operating system or
the cluster manager. For example, editing a text file in Vim on a supercomputer
would be terrible efficient resource utilization since editing a text file
requires only a small fraction of the supercomputer's available CPU and memory,
meaning the unneeded resources are wasted. In contrast, running a web browser on
a laptop is proper efficient resource utilization, because a web browser requires
an appropriate percentage of the laptop's available resources.
In the context of Kubernetes, an application with
poor efficient resource utilization would be a web server
that uses many replica pods, each reserving considerable resources,
to serve a very low volume of web requests. In such a situation, the resources
reserved for the application would be entirely underutilized.

Specifically, we measure efficient resource utilization with respect to
Kubernetes through examining the percentage of idle CPU.
The amount of CPU that a pod reserves is the summation of
all resources that containers within the pod reserve
\cite{k8s-compute-resources}. If our application is only using a small amount of
that reserved CPU to run, than a large amount of CPU will be left idle. The
larger the percentage of CPU that is left idle, the worse the efficient resource
utilization, as many resources are just sitting unused.
We measure our specific metric for efficient resource
utilization in percentage of CPU that is idle and we name it
\code{IdleCPU}. When we use this metric to indicate efficient resource utilization,
if the ERU value is high, the application is not using resources efficiently. If
the metric is low, the application is using resources efficiently.\footnote{In a
similar vein, a low measure for quality of service is actually preferable to a
high value for quality of service when we are measuring quality of service
through request response time. However, when summing ERU and QoS we consider
the inversion of this measurement of ERU, meaning a large summation of ERU and
QoS is preferable to a small summation of ERU and QoS.}

Our decision to use idle load to measure efficient resource utilization
in Kubernetes makes the assumption that
creating pods equates to reserving the pod's resources such
that they cannot be used by other pods.
In the default case, the previous statement is not necessarily true,
yet it is possible to craft specialized pods which validate this equality. To
start, remember that pods contain containers. When Kubernetes receives a pod, it
seeks to schedule all of its containers on a physical node within the cluster.
By default, containers within the pod run with no bounds on their CPU and memory
beyond the constrictions of the physical node on which they are scheduled. As such,
declaring $x$ number of pods does not give any guarantees of resource usage, as
the amount of resources available to the containers within the pod vary
drastically based on their specific node \cite{k8s-limit-range}. Without
modification, this variability would undermine \code{IdleCPU} as our metric for
ERU.

Fortunately, there is a way to configure Kubernetes such that a pod
equates to a static number of resources.\footnote{Right now in Kubernetes,
resources relates to either CPU or memory. CPU is requested in cores. Memory is
requested in bytes of RAM.} This configuration involves
setting resource requests and limits for each container within the pod. A
resource request for a container indicates the minimum amount of resources that
should always be available. A pod will not be scheduled on a node within the
cluster, unless that node can guarantee the requested amount of resources to all
containers within the pod. A resource limit for a container indicates the
maximum amount of resources that a container can claim. Depending on the
resource, a container exceeding the maximum amount of resources will either be
throttled (CPU) or killed (memory).\footnote{It is also possible to configure
Kubernetes such that a container using too much CPU is killed.} A pod's resource
request/limit is the summation of the resource request/limit for all of its
containers. Setting a container's, or pod's, resource request equal to its
resource limit essentially guarantees that the existence of a pod represents the
claiming and utilization of a static amount of resources
\cite{k8s-compute-resources}. Ensuring static provisioning
is reserving a consistent amount of resources
allows us to still examine idle CPU percentage, our way of investigating
efficient resource utilization.

The efficient resource utilization metric has direct links to the costs of
running applications on a cluster manager. If applications are given resources
they do not need, and the cluster manager does not reclaim these unused
resources, then additional applications added to the cluster must claim new
resources. The inability to utilize inefficient applications' wasted resources
requires the expansion of the cluster. This increase in cost will be felt
both by those running the cluster and those running an application on the
cluster.
