Given the powerful tooling described in the previous section, the process for
running a set of tests is completely automated and quite simple. Each test
must specify a traffic pattern, a scaling method, and a pod initialization time.
These variables influence the \textit{ReplicationController} and thus the
appropriate configuration file must be utilized.

We do this selection using environment variables, which allow us to run the
tests with the following single \textit{make} command.

\begin{minted}{bash}
  export TS_RC=test-server-controller-reactive-135s.yaml;
  export TG_RC=traffic-generator-increase-decrease-test-plan.yaml;
  export HPA=TRUE;
  export PREDICTIVE=TRUE;
  make test_start
\end{minted}

The above command indicates a wish to start a test instance with reactive
auto-scaling, a 135s pod initialization time, and an
\textit{increase-decrease} traffic pattern. It also rebuilds all containerized
applications and ensures the configuration files are up to date.

Once complete, all of the pods, replication controllers, services, and
auto-scalers on Kubernetes can be destroyed with the following \textit{make}
command.

\begin{minted}{bash}
  make test_stop
\end{minted}

As such, running a single test requires very little human involvement. The only
task is monitoring the tests to ensure they are no errors. Particularly, we are
concerned about writing our metrics to the database failing, Kubernetes
running out of space to schedule replica pods, and potential errors in our
predictive auto-scaling implementation. Fortunately, either Kubernetes provides,
or we have implemented, methods for highlighting such errors and making the
necessary adjustments to the evaluation process. In addition, after all the
tests run, we are able to determine what percent of requests to
\textit{test-server} were successful, and take action accordingly.
